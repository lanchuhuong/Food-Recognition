{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanchuhuong/Food-Recognition/blob/main/Visiontransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HkMkZ7IPK3h"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install -q torchbearer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPegf9TRPbjk"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y kaggle #need this to use kaggle api\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vi6MUePPkSF"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"lanchu\",\"key\":\"004e7e514e204a75f54db6f58796d439\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c food-recognition-challenge-2021 -p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWIzna8nP1RY",
        "outputId": "06f53375-b94a-485a-9294-99bf19377db5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace data/sample.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q data/food-recognition-challenge-2021.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeBhgiAFQQMD",
        "outputId": "d23188a5-da77-43bd-dac6-6d9c50459ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing, metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "pd.set_option('display.max_columns', 50)\n",
        "import re\n",
        "import timeit\n",
        "import random\n",
        "random.seed(3)\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import r2_score\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "import re\n",
        "import math\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import skimage.transform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "import torchbearer\n",
        "from torchbearer.callbacks import imaging\n",
        "from torchvision import transforms\n",
        "from torchbearer.cv_utils import DatasetValidationSplitter\n",
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torchbearer.callbacks import imaging\n",
        "from transformers import AutoFeatureExtractor, AutoModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "rngsZqsnc5cX",
        "outputId": "1af0ec14-bd06-4118-ab98-6148d408bd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght of dictionary: 30612\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>labels</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>489</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        images\n",
              "labels        \n",
              "1          474\n",
              "10         461\n",
              "11         392\n",
              "12         417\n",
              "13         286\n",
              "...        ...\n",
              "78         274\n",
              "79         421\n",
              "8          409\n",
              "80         369\n",
              "9          489\n",
              "\n",
              "[80 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "labels = pd.read_csv('data/train_labels.csv').set_index('img_name')\n",
        "labels = labels.to_dict()\n",
        "labels = labels[list(labels.keys())[0]]\n",
        "print(f'lenght of dictionary: {len(labels)}')\n",
        "df = pd.DataFrame(list(labels.items()))\n",
        "df.columns = ['images', 'labels']\n",
        "df = df.astype({'labels': str})\n",
        "# Display the occurance of each class\n",
        "df.groupby('labels').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPPfMXKyP7FH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "os.makedirs(\"data/train\", exist_ok=True)\n",
        "# os.makedirs(\"data/val\", exist_ok=True)\n",
        "\n",
        "for label in df['labels'].unique():\n",
        "  os.makedirs(f\"data/train/{label}\", exist_ok=True)\n",
        "  # os.makedirs(f\"data/val/{label}\", exist_ok=True)\n",
        "\n",
        "for image, label in df.values:\n",
        "  copyfile(f\"data/train_set/train_set/{image}\", f\"data/train/{label}/{image}\")\n",
        "  # copyfile(f\"data/val_set/val_set/{image}\", f\"data/val/{label}/{image}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFyUQyItllS3"
      },
      "outputs": [],
      "source": [
        "input_size = 224\n",
        "batch_size = 32\n",
        "#Data augmentation:\n",
        "#1.Flipping: flip each image horizontally by a chance of 50% (transforms.RandomHorizontalFlip).\n",
        "#2. transforms.RandomResizedCrop. This transformation scales the image in a small range, while eventually changing the aspect ratio, and crops it afterward in the previous size. Therefore, the actual pixel values change while the content or overall semantics of the image stays the same.\n",
        "transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                transforms.ToTensor(), transforms.RandomResizedCrop(input_size)])#resize the image to the input size, the model was trained on size 244*244\n",
        "\n",
        "train_data = datasets.ImageFolder('data/train', transform=transform)\n",
        "train_data, val_data = torch.utils.data.random_split(train_data,[25000,5612])\n",
        "\n",
        "trainset = torch.utils.data.DataLoader(train_data, pin_memory=True, batch_size=batch_size, shuffle=True)\n",
        "valset = torch.utils.data.DataLoader(val_data, pin_memory=True, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "T6IwiQ4_K_LI",
        "outputId": "3d02fc7c-a9bb-4c7a-d505-adf9055cf869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:22, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-55b77f07c513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;31m# pred = torch.max(outputs.data, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0;31m#correct_values = (pred == labels).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m           \u001b[0mcorrect_values\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0;31m#correct_values += (outputs.argmax(dim=-1) == labels).sum().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'float'"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"nateraw/food\")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.classifier = nn.Linear(768, 80)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    for ttype, dataset in zip([\"train\", \"val\"], [trainset, valset]):\n",
        "      running_loss = 0.0\n",
        "      correct_values = 0.0\n",
        "      count = 0.0\n",
        "      for i, data in tqdm(enumerate(dataset)):\n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          inputs, labels = data\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          if ttype=='train':\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "          outputs = model(inputs)\n",
        "          loss = loss_fun(outputs['logits'], labels)\n",
        "          if ttype=='train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          # pred = torch.max(outputs.data, 1)\n",
        "          #correct_values = (pred == labels).sum()\n",
        "          correct_values += (outputs == labels).float().sum()\n",
        "          #correct_values += (outputs.argmax(dim=-1) == labels).sum().item()\n",
        "          count += labels.shape[0]\n",
        "          running_loss += loss.item\n",
        "      #loss = running_loss / len(dataset)\n",
        "      loss = running_loss / count\n",
        "      #accuracy = correct_values / len(dataset)\n",
        "      accuracy = correct_values / count\n",
        "      if ttype == 'train':\n",
        "        train_acc.append(accuracy)\n",
        "        train_loss.append(loss)\n",
        "      if ttype == 'val':\n",
        "        val_acc.append(accuracy)\n",
        "        val_loss.append(loss)\n",
        "\n",
        "      print(f\"epoch {epoch}: train_acc = {train_acc} | train_loss = {train_loss} | val_acc = {val_acc} | val_loss = {val_loss}\")\n",
        "# print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zz-dEccc6FjZ"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'input_size': 784,\n",
        "              'output_size': 10,\n",
        "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Visionmodel.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsqtn4sXNgN5oxLSqt3w91",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}